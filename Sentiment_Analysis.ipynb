{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import pos_tag\n",
    "from math import floor\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('Twitter_dataset.csv')\n",
    "# del data['Flag']\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# # data.head()\n",
    "\n",
    "# data['target'][data['target']==4]=1\n",
    "\n",
    "# data.head()\n",
    "\n",
    "# data.to_csv('Twitter_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to size of dataset, I was only able to upload 400000 rows of this dataset. Hence, in order to recreate this model, this minor adjustment is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Twitter_dataset.csv', nrows=500000)\n",
    "\n",
    "data.drop_duplicates(subset=['Text'],inplace=True)\n",
    "data.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r'\\([^)]*\\)','',newString)\n",
    "    newString = ''.join((contraction_mapping[t]+' ') if t in contraction_mapping else (t+' ') for t in newString.split(\" \"))\n",
    "    newString = re.sub(r'\"','',newString)\n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z\\d]\", \" \", newString)\n",
    "    tokens = ''.join((w+' ') if w not in stop_words else '' for w in newString.split())\n",
    "    return newString\n",
    "\n",
    "cleaned_text = []\n",
    "max_words = 0\n",
    "for i in data['Text']:\n",
    "    max_words = max(max_words,len(i))\n",
    "    cleaned_text.append(text_cleaner(i))\n",
    "data['Cleaned Text'] = cleaned_text\n",
    "print(max_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>IDs</th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2185778769</td>\n",
       "      <td>Mon Jun 15 17:53:33 PDT 2009</td>\n",
       "      <td>TuckerPeterson</td>\n",
       "      <td>@JoeRinaldi They can do anything with CGI nowa...</td>\n",
       "      <td>joerinaldi they can do anything with cgi nowa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2053725693</td>\n",
       "      <td>Sat Jun 06 05:30:43 PDT 2009</td>\n",
       "      <td>Curly_Wurlies</td>\n",
       "      <td>@silentbobb oh totally, thats the plan  yooh?</td>\n",
       "      <td>silentbobb oh totally  thats the plan  yooh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2251814388</td>\n",
       "      <td>Sat Jun 20 04:39:41 PDT 2009</td>\n",
       "      <td>rbmartin</td>\n",
       "      <td>@smokey_2009 I had to!</td>\n",
       "      <td>smokey 2009 i had to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2264180979</td>\n",
       "      <td>Sun Jun 21 03:02:31 PDT 2009</td>\n",
       "      <td>bwicaksono</td>\n",
       "      <td>I'm just testing my 1st augmented reality apps...</td>\n",
       "      <td>i am just testing my 1st augmented reality app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2222352562</td>\n",
       "      <td>Thu Jun 18 06:51:47 PDT 2009</td>\n",
       "      <td>jesusowns9</td>\n",
       "      <td>I forgot my phone at the Vinskies'  i feel so ...</td>\n",
       "      <td>i forgot my phone at the vinskies   i feel so ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         IDs                          Date            User  \\\n",
       "0       1  2185778769  Mon Jun 15 17:53:33 PDT 2009  TuckerPeterson   \n",
       "1       1  2053725693  Sat Jun 06 05:30:43 PDT 2009   Curly_Wurlies   \n",
       "2       0  2251814388  Sat Jun 20 04:39:41 PDT 2009        rbmartin   \n",
       "3       0  2264180979  Sun Jun 21 03:02:31 PDT 2009      bwicaksono   \n",
       "4       0  2222352562  Thu Jun 18 06:51:47 PDT 2009      jesusowns9   \n",
       "\n",
       "                                                Text  \\\n",
       "0  @JoeRinaldi They can do anything with CGI nowa...   \n",
       "1      @silentbobb oh totally, thats the plan  yooh?   \n",
       "2                           @smokey_2009 I had to!     \n",
       "3  I'm just testing my 1st augmented reality apps...   \n",
       "4  I forgot my phone at the Vinskies'  i feel so ...   \n",
       "\n",
       "                                        Cleaned Text  \n",
       "0   joerinaldi they can do anything with cgi nowa...  \n",
       "1      silentbobb oh totally  thats the plan  yooh    \n",
       "2                           smokey 2009 i had to      \n",
       "3  i am just testing my 1st augmented reality app...  \n",
       "4  i forgot my phone at the vinskies   i feel so ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0, 0, 0])\n",
      "torch.Size([96569])\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(data['Cleaned Text'])[:400000]\n",
    "Y_train = np.array(data['target'])[:400000]\n",
    "# Y_train = [[0,1] if w==0 else [1,0] for w in Y_train]\n",
    "Y_train = torch.tensor(Y_train)\n",
    "print(Y_train[0:5])\n",
    "\n",
    "X_test = np.array(data['Cleaned Text'])[400000:]\n",
    "Y_test = np.array(data['target'])[400000:]\n",
    "# Y_test = [[0,1] if w==0 else [1,0] for w in Y_test]\n",
    "Y_test = torch.tensor(Y_test)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "global x_train_tokens\n",
    "x_train_tokens = x_tokenizer.texts_to_sequences(X_train)\n",
    "x_test_tokens = x_tokenizer.texts_to_sequences(X_test)\n",
    "x_train_tokens = pad_sequences(x_train_tokens,  maxlen=max_words, padding='post')\n",
    "x_test_tokens = pad_sequences(x_test_tokens,  maxlen=max_words, padding='post')\n",
    "\n",
    "\n",
    "x_train_tokens = torch.tensor(x_train_tokens)\n",
    "x_test_tokens = torch.tensor(x_test_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,   472,     6,   214,    26,     3, 76844,     1,   109,    19,\n",
      "         1416,   379,     8,     1,   251,    10,  1798,  1407,    42,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tokens[4])\n",
    "x_train_tokens = x_train_tokens.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-No: 1\n",
      "Batch: 1 / 4000 Running Loss: 0.7 Running Accuracy: 53.0\n",
      "Batch: 101 / 4000 Running Loss: 0.69 Running Accuracy: 55.8\n",
      "Batch: 201 / 4000 Running Loss: 0.69 Running Accuracy: 55.75\n",
      "Batch: 301 / 4000 Running Loss: 0.69 Running Accuracy: 55.7\n",
      "Batch: 401 / 4000 Running Loss: 0.68 Running Accuracy: 55.96\n",
      "Batch: 501 / 4000 Running Loss: 0.68 Running Accuracy: 55.94\n",
      "Batch: 601 / 4000 Running Loss: 0.68 Running Accuracy: 56.16\n",
      "Batch: 701 / 4000 Running Loss: 0.68 Running Accuracy: 56.2\n",
      "Batch: 801 / 4000 Running Loss: 0.68 Running Accuracy: 56.26\n",
      "Batch: 901 / 4000 Running Loss: 0.68 Running Accuracy: 56.42\n",
      "Batch: 1001 / 4000 Running Loss: 0.68 Running Accuracy: 56.44\n",
      "Batch: 1101 / 4000 Running Loss: 0.68 Running Accuracy: 56.47\n",
      "Batch: 1201 / 4000 Running Loss: 0.68 Running Accuracy: 56.46\n",
      "Batch: 1301 / 4000 Running Loss: 0.68 Running Accuracy: 56.5\n",
      "Batch: 1401 / 4000 Running Loss: 0.68 Running Accuracy: 56.58\n",
      "Batch: 1501 / 4000 Running Loss: 0.68 Running Accuracy: 56.62\n",
      "Batch: 1601 / 4000 Running Loss: 0.68 Running Accuracy: 56.62\n",
      "Batch: 1701 / 4000 Running Loss: 0.68 Running Accuracy: 56.67\n",
      "Batch: 1801 / 4000 Running Loss: 0.68 Running Accuracy: 56.72\n",
      "Batch: 1901 / 4000 Running Loss: 0.68 Running Accuracy: 56.78\n",
      "Batch: 2001 / 4000 Running Loss: 0.68 Running Accuracy: 56.76\n",
      "Batch: 2101 / 4000 Running Loss: 0.68 Running Accuracy: 56.78\n",
      "Batch: 2201 / 4000 Running Loss: 0.68 Running Accuracy: 56.8\n",
      "Batch: 2301 / 4000 Running Loss: 0.68 Running Accuracy: 56.83\n",
      "Batch: 2401 / 4000 Running Loss: 0.68 Running Accuracy: 56.84\n",
      "Batch: 2501 / 4000 Running Loss: 0.68 Running Accuracy: 56.84\n",
      "Batch: 2601 / 4000 Running Loss: 0.68 Running Accuracy: 56.84\n",
      "Batch: 2701 / 4000 Running Loss: 0.68 Running Accuracy: 56.87\n",
      "Batch: 2801 / 4000 Running Loss: 0.68 Running Accuracy: 56.89\n",
      "Batch: 2901 / 4000 Running Loss: 0.68 Running Accuracy: 56.92\n",
      "Batch: 3001 / 4000 Running Loss: 0.68 Running Accuracy: 56.91\n",
      "Batch: 3101 / 4000 Running Loss: 0.68 Running Accuracy: 56.93\n",
      "Batch: 3201 / 4000 Running Loss: 0.68 Running Accuracy: 56.95\n",
      "Batch: 3301 / 4000 Running Loss: 0.68 Running Accuracy: 56.97\n",
      "Batch: 3401 / 4000 Running Loss: 0.68 Running Accuracy: 56.99\n",
      "Batch: 3501 / 4000 Running Loss: 0.68 Running Accuracy: 56.97\n",
      "Batch: 3601 / 4000 Running Loss: 0.68 Running Accuracy: 56.98\n",
      "Batch: 3701 / 4000 Running Loss: 0.68 Running Accuracy: 56.99\n",
      "Batch: 3801 / 4000 Running Loss: 0.68 Running Accuracy: 56.98\n",
      "Batch: 3901 / 4000 Running Loss: 0.68 Running Accuracy: 56.98\n",
      "Batch: 1 / 4000 Running Loss: 0.68 Running Accuracy: 55.0\n",
      "Batch: 101 / 4000 Running Loss: 0.68 Running Accuracy: 57.52\n",
      "Batch: 201 / 4000 Running Loss: 0.68 Running Accuracy: 57.26\n",
      "Batch: 301 / 4000 Running Loss: 0.68 Running Accuracy: 57.04\n",
      "Batch: 401 / 4000 Running Loss: 0.68 Running Accuracy: 57.2\n",
      "Batch: 501 / 4000 Running Loss: 0.68 Running Accuracy: 57.14\n",
      "Batch: 601 / 4000 Running Loss: 0.68 Running Accuracy: 57.35\n",
      "Batch: 701 / 4000 Running Loss: 0.68 Running Accuracy: 57.37\n",
      "Batch: 801 / 4000 Running Loss: 0.68 Running Accuracy: 57.32\n",
      "Batch: 901 / 4000 Running Loss: 0.68 Running Accuracy: 57.36\n",
      "Batch: 1001 / 4000 Running Loss: 0.68 Running Accuracy: 57.37\n",
      "Batch: 1101 / 4000 Running Loss: 0.68 Running Accuracy: 57.31\n",
      "Batch: 1201 / 4000 Running Loss: 0.68 Running Accuracy: 57.29\n",
      "Batch: 1301 / 4000 Running Loss: 0.68 Running Accuracy: 57.32\n",
      "Batch: 1401 / 4000 Running Loss: 0.68 Running Accuracy: 57.35\n",
      "Batch: 1501 / 4000 Running Loss: 0.68 Running Accuracy: 57.34\n",
      "Batch: 1601 / 4000 Running Loss: 0.68 Running Accuracy: 57.3\n",
      "Batch: 1701 / 4000 Running Loss: 0.68 Running Accuracy: 57.34\n",
      "Batch: 1801 / 4000 Running Loss: 0.68 Running Accuracy: 57.34\n",
      "Batch: 1901 / 4000 Running Loss: 0.68 Running Accuracy: 57.36\n",
      "Batch: 2001 / 4000 Running Loss: 0.68 Running Accuracy: 57.31\n",
      "Batch: 2101 / 4000 Running Loss: 0.68 Running Accuracy: 57.3\n",
      "Batch: 2201 / 4000 Running Loss: 0.68 Running Accuracy: 57.29\n",
      "Batch: 2301 / 4000 Running Loss: 0.68 Running Accuracy: 57.3\n",
      "Batch: 2401 / 4000 Running Loss: 0.68 Running Accuracy: 57.28\n",
      "Batch: 2501 / 4000 Running Loss: 0.68 Running Accuracy: 57.26\n",
      "Batch: 2601 / 4000 Running Loss: 0.68 Running Accuracy: 57.26\n",
      "Batch: 2701 / 4000 Running Loss: 0.68 Running Accuracy: 57.3\n",
      "Batch: 2801 / 4000 Running Loss: 0.68 Running Accuracy: 57.31\n",
      "Batch: 2901 / 4000 Running Loss: 0.68 Running Accuracy: 57.32\n",
      "Batch: 3001 / 4000 Running Loss: 0.68 Running Accuracy: 57.31\n",
      "Batch: 3101 / 4000 Running Loss: 0.68 Running Accuracy: 57.3\n",
      "Batch: 3201 / 4000 Running Loss: 0.68 Running Accuracy: 57.3\n",
      "Batch: 3301 / 4000 Running Loss: 0.68 Running Accuracy: 57.32\n",
      "Batch: 3401 / 4000 Running Loss: 0.68 Running Accuracy: 57.34\n",
      "Batch: 3501 / 4000 Running Loss: 0.68 Running Accuracy: 57.33\n",
      "Batch: 3601 / 4000 Running Loss: 0.68 Running Accuracy: 57.33\n",
      "Batch: 3701 / 4000 Running Loss: 0.68 Running Accuracy: 57.34\n",
      "Batch: 3801 / 4000 Running Loss: 0.68 Running Accuracy: 57.33\n",
      "Batch: 3901 / 4000 Running Loss: 0.68 Running Accuracy: 57.34\n",
      "Training: Epoch Loss: 0.68 Epoch Accuracy: 56.98\n",
      "Inference: Epoch Loss: 0.68 Epoch Accuracy: 57.33\n",
      "--------------------------------------------------\n",
      "Epoch-No: 2\n",
      "Batch: 1 / 4000 Running Loss: 0.67 Running Accuracy: 56.0\n",
      "Batch: 101 / 4000 Running Loss: 0.68 Running Accuracy: 57.3\n",
      "Batch: 201 / 4000 Running Loss: 0.68 Running Accuracy: 57.16\n",
      "Batch: 301 / 4000 Running Loss: 0.68 Running Accuracy: 57.15\n",
      "Batch: 401 / 4000 Running Loss: 0.68 Running Accuracy: 57.34\n",
      "Batch: 501 / 4000 Running Loss: 0.68 Running Accuracy: 57.25\n",
      "Batch: 601 / 4000 Running Loss: 0.68 Running Accuracy: 57.45\n",
      "Batch: 701 / 4000 Running Loss: 0.68 Running Accuracy: 57.46\n",
      "Batch: 801 / 4000 Running Loss: 0.68 Running Accuracy: 57.46\n",
      "Batch: 901 / 4000 Running Loss: 0.68 Running Accuracy: 57.53\n",
      "Batch: 1001 / 4000 Running Loss: 0.68 Running Accuracy: 57.53\n",
      "Batch: 1101 / 4000 Running Loss: 0.68 Running Accuracy: 57.5\n",
      "Batch: 1201 / 4000 Running Loss: 0.68 Running Accuracy: 57.45\n",
      "Batch: 1301 / 4000 Running Loss: 0.68 Running Accuracy: 57.45\n",
      "Batch: 1401 / 4000 Running Loss: 0.68 Running Accuracy: 57.48\n",
      "Batch: 1501 / 4000 Running Loss: 0.68 Running Accuracy: 57.48\n",
      "Batch: 1601 / 4000 Running Loss: 0.68 Running Accuracy: 57.46\n",
      "Batch: 1701 / 4000 Running Loss: 0.68 Running Accuracy: 57.5\n",
      "Batch: 1801 / 4000 Running Loss: 0.68 Running Accuracy: 57.52\n",
      "Batch: 1901 / 4000 Running Loss: 0.68 Running Accuracy: 57.55\n",
      "Batch: 2001 / 4000 Running Loss: 0.68 Running Accuracy: 57.5\n",
      "Batch: 2101 / 4000 Running Loss: 0.68 Running Accuracy: 57.49\n",
      "Batch: 2201 / 4000 Running Loss: 0.68 Running Accuracy: 57.49\n",
      "Batch: 2301 / 4000 Running Loss: 0.68 Running Accuracy: 57.5\n",
      "Batch: 2401 / 4000 Running Loss: 0.68 Running Accuracy: 57.48\n",
      "Batch: 2501 / 4000 Running Loss: 0.68 Running Accuracy: 57.46\n",
      "Batch: 2601 / 4000 Running Loss: 0.68 Running Accuracy: 57.45\n",
      "Batch: 2701 / 4000 Running Loss: 0.68 Running Accuracy: 57.49\n",
      "Batch: 2801 / 4000 Running Loss: 0.68 Running Accuracy: 57.48\n",
      "Batch: 2901 / 4000 Running Loss: 0.68 Running Accuracy: 57.5\n",
      "Batch: 3001 / 4000 Running Loss: 0.68 Running Accuracy: 57.5\n",
      "Batch: 3101 / 4000 Running Loss: 0.68 Running Accuracy: 57.5\n",
      "Batch: 3201 / 4000 Running Loss: 0.68 Running Accuracy: 57.51\n",
      "Batch: 3301 / 4000 Running Loss: 0.68 Running Accuracy: 57.52\n",
      "Batch: 3401 / 4000 Running Loss: 0.68 Running Accuracy: 57.53\n",
      "Batch: 3501 / 4000 Running Loss: 0.68 Running Accuracy: 57.51\n",
      "Batch: 3601 / 4000 Running Loss: 0.68 Running Accuracy: 57.5\n",
      "Batch: 3701 / 4000 Running Loss: 0.68 Running Accuracy: 57.5\n",
      "Batch: 3801 / 4000 Running Loss: 0.68 Running Accuracy: 57.48\n",
      "Batch: 3901 / 4000 Running Loss: 0.68 Running Accuracy: 57.48\n",
      "Batch: 1 / 4000 Running Loss: 0.67 Running Accuracy: 58.0\n",
      "Batch: 101 / 4000 Running Loss: 0.68 Running Accuracy: 57.67\n",
      "Batch: 201 / 4000 Running Loss: 0.68 Running Accuracy: 57.54\n",
      "Batch: 301 / 4000 Running Loss: 0.68 Running Accuracy: 57.4\n",
      "Batch: 401 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 501 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 601 / 4000 Running Loss: 0.68 Running Accuracy: 57.75\n",
      "Batch: 701 / 4000 Running Loss: 0.68 Running Accuracy: 57.7\n",
      "Batch: 801 / 4000 Running Loss: 0.68 Running Accuracy: 57.67\n",
      "Batch: 901 / 4000 Running Loss: 0.68 Running Accuracy: 57.7\n",
      "Batch: 1001 / 4000 Running Loss: 0.68 Running Accuracy: 57.73\n",
      "Batch: 1101 / 4000 Running Loss: 0.68 Running Accuracy: 57.68\n",
      "Batch: 1201 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 1301 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 1401 / 4000 Running Loss: 0.68 Running Accuracy: 57.69\n",
      "Batch: 1501 / 4000 Running Loss: 0.68 Running Accuracy: 57.7\n",
      "Batch: 1601 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 1701 / 4000 Running Loss: 0.68 Running Accuracy: 57.69\n",
      "Batch: 1801 / 4000 Running Loss: 0.68 Running Accuracy: 57.7\n",
      "Batch: 1901 / 4000 Running Loss: 0.68 Running Accuracy: 57.72\n",
      "Batch: 2001 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 2101 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 2201 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 2301 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 2401 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 2501 / 4000 Running Loss: 0.68 Running Accuracy: 57.57\n",
      "Batch: 2601 / 4000 Running Loss: 0.68 Running Accuracy: 57.56\n",
      "Batch: 2701 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 2801 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 2901 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3001 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 3101 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 3201 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3301 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 3401 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 3501 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3601 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 3701 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3801 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 3901 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Training: Epoch Loss: 0.68 Epoch Accuracy: 57.47\n",
      "Inference: Epoch Loss: 0.68 Epoch Accuracy: 57.58\n",
      "--------------------------------------------------\n",
      "Epoch-No: 3\n",
      "Batch: 1 / 4000 Running Loss: 0.67 Running Accuracy: 60.0\n",
      "Batch: 101 / 4000 Running Loss: 0.68 Running Accuracy: 57.25\n",
      "Batch: 201 / 4000 Running Loss: 0.68 Running Accuracy: 57.1\n",
      "Batch: 301 / 4000 Running Loss: 0.68 Running Accuracy: 57.14\n",
      "Batch: 401 / 4000 Running Loss: 0.68 Running Accuracy: 57.4\n",
      "Batch: 501 / 4000 Running Loss: 0.68 Running Accuracy: 57.41\n",
      "Batch: 601 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 701 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 801 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 901 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 1001 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 1101 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 1201 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 1301 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 1401 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 1501 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 1601 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 1701 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 1801 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 1901 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 2001 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 2101 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 2201 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 2301 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 2401 / 4000 Running Loss: 0.68 Running Accuracy: 57.57\n",
      "Batch: 2501 / 4000 Running Loss: 0.68 Running Accuracy: 57.54\n",
      "Batch: 2601 / 4000 Running Loss: 0.68 Running Accuracy: 57.53\n",
      "Batch: 2701 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 2801 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 2901 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 3001 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 3101 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 3201 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 3301 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3401 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 3501 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 3601 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 3701 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 3801 / 4000 Running Loss: 0.68 Running Accuracy: 57.57\n",
      "Batch: 3901 / 4000 Running Loss: 0.68 Running Accuracy: 57.57\n",
      "Batch: 1 / 4000 Running Loss: 0.67 Running Accuracy: 60.0\n",
      "Batch: 101 / 4000 Running Loss: 0.68 Running Accuracy: 57.77\n",
      "Batch: 201 / 4000 Running Loss: 0.68 Running Accuracy: 57.54\n",
      "Batch: 301 / 4000 Running Loss: 0.68 Running Accuracy: 57.39\n",
      "Batch: 401 / 4000 Running Loss: 0.68 Running Accuracy: 57.56\n",
      "Batch: 501 / 4000 Running Loss: 0.68 Running Accuracy: 57.53\n",
      "Batch: 601 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 701 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 801 / 4000 Running Loss: 0.68 Running Accuracy: 57.57\n",
      "Batch: 901 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 1001 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 1101 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 1201 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 1301 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 1401 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 1501 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 1601 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 1701 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 1801 / 4000 Running Loss: 0.68 Running Accuracy: 57.67\n",
      "Batch: 1901 / 4000 Running Loss: 0.68 Running Accuracy: 57.68\n",
      "Batch: 2001 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 2101 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 2201 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 2301 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 2401 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 2501 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 2601 / 4000 Running Loss: 0.68 Running Accuracy: 57.57\n",
      "Batch: 2701 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 2801 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 2901 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 3001 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 3101 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 3201 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 3301 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 3401 / 4000 Running Loss: 0.68 Running Accuracy: 57.67\n",
      "Batch: 3501 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 3601 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 3701 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 3801 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 3901 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Training: Epoch Loss: 0.68 Epoch Accuracy: 57.55\n",
      "Inference: Epoch Loss: 0.68 Epoch Accuracy: 57.65\n",
      "--------------------------------------------------\n",
      "Epoch-No: 4\n",
      "Batch: 1 / 4000 Running Loss: 0.66 Running Accuracy: 61.0\n",
      "Batch: 101 / 4000 Running Loss: 0.68 Running Accuracy: 57.47\n",
      "Batch: 201 / 4000 Running Loss: 0.68 Running Accuracy: 57.2\n",
      "Batch: 301 / 4000 Running Loss: 0.68 Running Accuracy: 57.2\n",
      "Batch: 401 / 4000 Running Loss: 0.68 Running Accuracy: 57.43\n",
      "Batch: 501 / 4000 Running Loss: 0.68 Running Accuracy: 57.41\n",
      "Batch: 601 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 701 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 801 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 901 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 1001 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 1101 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 1201 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 1301 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 1401 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 1501 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 1601 / 4000 Running Loss: 0.68 Running Accuracy: 57.57\n",
      "Batch: 1701 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 1801 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 1901 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 2001 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 2101 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 2201 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 2301 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 2401 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 2501 / 4000 Running Loss: 0.68 Running Accuracy: 57.56\n",
      "Batch: 2601 / 4000 Running Loss: 0.68 Running Accuracy: 57.56\n",
      "Batch: 2701 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 2801 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 2901 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3001 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3101 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3201 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3301 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 3401 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 3501 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3601 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3701 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3801 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 3901 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 1 / 4000 Running Loss: 0.67 Running Accuracy: 60.0\n",
      "Batch: 101 / 4000 Running Loss: 0.68 Running Accuracy: 57.32\n",
      "Batch: 201 / 4000 Running Loss: 0.68 Running Accuracy: 57.27\n",
      "Batch: 301 / 4000 Running Loss: 0.68 Running Accuracy: 57.2\n",
      "Batch: 401 / 4000 Running Loss: 0.68 Running Accuracy: 57.33\n",
      "Batch: 501 / 4000 Running Loss: 0.68 Running Accuracy: 57.34\n",
      "Batch: 601 / 4000 Running Loss: 0.68 Running Accuracy: 57.5\n",
      "Batch: 701 / 4000 Running Loss: 0.68 Running Accuracy: 57.49\n",
      "Batch: 801 / 4000 Running Loss: 0.68 Running Accuracy: 57.49\n",
      "Batch: 901 / 4000 Running Loss: 0.68 Running Accuracy: 57.54\n",
      "Batch: 1001 / 4000 Running Loss: 0.68 Running Accuracy: 57.56\n",
      "Batch: 1101 / 4000 Running Loss: 0.68 Running Accuracy: 57.52\n",
      "Batch: 1201 / 4000 Running Loss: 0.68 Running Accuracy: 57.51\n",
      "Batch: 1301 / 4000 Running Loss: 0.68 Running Accuracy: 57.52\n",
      "Batch: 1401 / 4000 Running Loss: 0.68 Running Accuracy: 57.57\n",
      "Batch: 1501 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 1601 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 1701 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 1801 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 1901 / 4000 Running Loss: 0.68 Running Accuracy: 57.68\n",
      "Batch: 2001 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 2101 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 2201 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 2301 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 2401 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 2501 / 4000 Running Loss: 0.68 Running Accuracy: 57.56\n",
      "Batch: 2601 / 4000 Running Loss: 0.68 Running Accuracy: 57.55\n",
      "Batch: 2701 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 2801 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 2901 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 3001 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 3101 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 3201 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 3301 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 3401 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 3501 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 3601 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 3701 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 3801 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 3901 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Training: Epoch Loss: 0.68 Epoch Accuracy: 57.57\n",
      "Inference: Epoch Loss: 0.68 Epoch Accuracy: 57.62\n",
      "--------------------------------------------------\n",
      "Epoch-No: 5\n",
      "Batch: 1 / 4000 Running Loss: 0.66 Running Accuracy: 62.0\n",
      "Batch: 101 / 4000 Running Loss: 0.68 Running Accuracy: 57.46\n",
      "Batch: 201 / 4000 Running Loss: 0.68 Running Accuracy: 57.25\n",
      "Batch: 301 / 4000 Running Loss: 0.68 Running Accuracy: 57.16\n",
      "Batch: 401 / 4000 Running Loss: 0.68 Running Accuracy: 57.35\n",
      "Batch: 501 / 4000 Running Loss: 0.68 Running Accuracy: 57.31\n",
      "Batch: 601 / 4000 Running Loss: 0.68 Running Accuracy: 57.55\n",
      "Batch: 701 / 4000 Running Loss: 0.68 Running Accuracy: 57.54\n",
      "Batch: 801 / 4000 Running Loss: 0.68 Running Accuracy: 57.53\n",
      "Batch: 901 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 1001 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 1101 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 1201 / 4000 Running Loss: 0.68 Running Accuracy: 57.57\n",
      "Batch: 1301 / 4000 Running Loss: 0.68 Running Accuracy: 57.57\n",
      "Batch: 1401 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 1501 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 1601 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 1701 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 1801 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 1901 / 4000 Running Loss: 0.68 Running Accuracy: 57.68\n",
      "Batch: 2001 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 2101 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 2201 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 2301 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 2401 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 2501 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 2601 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 2701 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 2801 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 2901 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 3001 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 3101 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 3201 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 3301 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 3401 / 4000 Running Loss: 0.68 Running Accuracy: 57.67\n",
      "Batch: 3501 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 3601 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 3701 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 3801 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 3901 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 1 / 4000 Running Loss: 0.67 Running Accuracy: 60.0\n",
      "Batch: 101 / 4000 Running Loss: 0.68 Running Accuracy: 57.5\n",
      "Batch: 201 / 4000 Running Loss: 0.68 Running Accuracy: 57.39\n",
      "Batch: 301 / 4000 Running Loss: 0.68 Running Accuracy: 57.31\n",
      "Batch: 401 / 4000 Running Loss: 0.68 Running Accuracy: 57.47\n",
      "Batch: 501 / 4000 Running Loss: 0.68 Running Accuracy: 57.43\n",
      "Batch: 601 / 4000 Running Loss: 0.68 Running Accuracy: 57.56\n",
      "Batch: 701 / 4000 Running Loss: 0.68 Running Accuracy: 57.53\n",
      "Batch: 801 / 4000 Running Loss: 0.68 Running Accuracy: 57.54\n",
      "Batch: 901 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 1001 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 1101 / 4000 Running Loss: 0.68 Running Accuracy: 57.57\n",
      "Batch: 1201 / 4000 Running Loss: 0.68 Running Accuracy: 57.55\n",
      "Batch: 1301 / 4000 Running Loss: 0.68 Running Accuracy: 57.56\n",
      "Batch: 1401 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 1501 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 1601 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 1701 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 1801 / 4000 Running Loss: 0.68 Running Accuracy: 57.68\n",
      "Batch: 1901 / 4000 Running Loss: 0.68 Running Accuracy: 57.71\n",
      "Batch: 2001 / 4000 Running Loss: 0.68 Running Accuracy: 57.67\n",
      "Batch: 2101 / 4000 Running Loss: 0.68 Running Accuracy: 57.67\n",
      "Batch: 2201 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 2301 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 2401 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 2501 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 2601 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 2701 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 2801 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 2901 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 3001 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 3101 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 3201 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 3301 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 3401 / 4000 Running Loss: 0.68 Running Accuracy: 57.67\n",
      "Batch: 3501 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 3601 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 3701 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 3801 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 3901 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Training: Epoch Loss: 0.68 Epoch Accuracy: 57.62\n",
      "Inference: Epoch Loss: 0.68 Epoch Accuracy: 57.64\n",
      "--------------------------------------------------\n",
      "Epoch-No: 6\n",
      "Batch: 1 / 4000 Running Loss: 0.66 Running Accuracy: 63.0\n",
      "Batch: 101 / 4000 Running Loss: 0.68 Running Accuracy: 57.75\n",
      "Batch: 201 / 4000 Running Loss: 0.68 Running Accuracy: 57.42\n",
      "Batch: 301 / 4000 Running Loss: 0.68 Running Accuracy: 57.26\n",
      "Batch: 401 / 4000 Running Loss: 0.68 Running Accuracy: 57.43\n",
      "Batch: 501 / 4000 Running Loss: 0.68 Running Accuracy: 57.39\n",
      "Batch: 601 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 701 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 801 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 901 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 1001 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 1101 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 1201 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 1301 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 1401 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 1501 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 1601 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 1701 / 4000 Running Loss: 0.68 Running Accuracy: 57.67\n",
      "Batch: 1801 / 4000 Running Loss: 0.68 Running Accuracy: 57.68\n",
      "Batch: 1901 / 4000 Running Loss: 0.68 Running Accuracy: 57.71\n",
      "Batch: 2001 / 4000 Running Loss: 0.68 Running Accuracy: 57.67\n",
      "Batch: 2101 / 4000 Running Loss: 0.68 Running Accuracy: 57.67\n",
      "Batch: 2201 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 2301 / 4000 Running Loss: 0.68 Running Accuracy: 57.68\n",
      "Batch: 2401 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 2501 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 2601 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 2701 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 2801 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 2901 / 4000 Running Loss: 0.68 Running Accuracy: 57.67\n",
      "Batch: 3001 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 3101 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 3201 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 3301 / 4000 Running Loss: 0.68 Running Accuracy: 57.68\n",
      "Batch: 3401 / 4000 Running Loss: 0.68 Running Accuracy: 57.69\n",
      "Batch: 3501 / 4000 Running Loss: 0.68 Running Accuracy: 57.67\n",
      "Batch: 3601 / 4000 Running Loss: 0.68 Running Accuracy: 57.67\n",
      "Batch: 3701 / 4000 Running Loss: 0.68 Running Accuracy: 57.68\n",
      "Batch: 3801 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 3901 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 1 / 4000 Running Loss: 0.66 Running Accuracy: 61.0\n",
      "Batch: 101 / 4000 Running Loss: 0.68 Running Accuracy: 57.5\n",
      "Batch: 201 / 4000 Running Loss: 0.68 Running Accuracy: 57.34\n",
      "Batch: 301 / 4000 Running Loss: 0.68 Running Accuracy: 57.27\n",
      "Batch: 401 / 4000 Running Loss: 0.68 Running Accuracy: 57.43\n",
      "Batch: 501 / 4000 Running Loss: 0.68 Running Accuracy: 57.39\n",
      "Batch: 601 / 4000 Running Loss: 0.68 Running Accuracy: 57.57\n",
      "Batch: 701 / 4000 Running Loss: 0.68 Running Accuracy: 57.52\n",
      "Batch: 801 / 4000 Running Loss: 0.68 Running Accuracy: 57.5\n",
      "Batch: 901 / 4000 Running Loss: 0.68 Running Accuracy: 57.54\n",
      "Batch: 1001 / 4000 Running Loss: 0.68 Running Accuracy: 57.57\n",
      "Batch: 1101 / 4000 Running Loss: 0.68 Running Accuracy: 57.54\n",
      "Batch: 1201 / 4000 Running Loss: 0.68 Running Accuracy: 57.5\n",
      "Batch: 1301 / 4000 Running Loss: 0.68 Running Accuracy: 57.53\n",
      "Batch: 1401 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 1501 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 1601 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 1701 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 1801 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 1901 / 4000 Running Loss: 0.68 Running Accuracy: 57.67\n",
      "Batch: 2001 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 2101 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 2201 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 2301 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 2401 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 2501 / 4000 Running Loss: 0.68 Running Accuracy: 57.57\n",
      "Batch: 2601 / 4000 Running Loss: 0.68 Running Accuracy: 57.56\n",
      "Batch: 2701 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 2801 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 2901 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3001 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3101 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3201 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3301 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 3401 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 3501 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 3601 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 3701 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 3801 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 3901 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Training: Epoch Loss: 0.68 Epoch Accuracy: 57.66\n",
      "Inference: Epoch Loss: 0.68 Epoch Accuracy: 57.62\n",
      "--------------------------------------------------\n",
      "Epoch-No: 7\n",
      "Batch: 1 / 4000 Running Loss: 0.66 Running Accuracy: 64.0\n",
      "Batch: 101 / 4000 Running Loss: 0.68 Running Accuracy: 57.72\n",
      "Batch: 201 / 4000 Running Loss: 0.68 Running Accuracy: 57.49\n",
      "Batch: 301 / 4000 Running Loss: 0.68 Running Accuracy: 57.37\n",
      "Batch: 401 / 4000 Running Loss: 0.68 Running Accuracy: 57.53\n",
      "Batch: 501 / 4000 Running Loss: 0.68 Running Accuracy: 57.5\n",
      "Batch: 601 / 4000 Running Loss: 0.68 Running Accuracy: 57.68\n",
      "Batch: 701 / 4000 Running Loss: 0.68 Running Accuracy: 57.67\n",
      "Batch: 801 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 901 / 4000 Running Loss: 0.68 Running Accuracy: 57.73\n",
      "Batch: 1001 / 4000 Running Loss: 0.68 Running Accuracy: 57.74\n",
      "Batch: 1101 / 4000 Running Loss: 0.68 Running Accuracy: 57.71\n",
      "Batch: 1201 / 4000 Running Loss: 0.68 Running Accuracy: 57.68\n",
      "Batch: 1301 / 4000 Running Loss: 0.68 Running Accuracy: 57.7\n",
      "Batch: 1401 / 4000 Running Loss: 0.68 Running Accuracy: 57.71\n",
      "Batch: 1501 / 4000 Running Loss: 0.68 Running Accuracy: 57.7\n",
      "Batch: 1601 / 4000 Running Loss: 0.68 Running Accuracy: 57.68\n",
      "Batch: 1701 / 4000 Running Loss: 0.68 Running Accuracy: 57.72\n",
      "Batch: 1801 / 4000 Running Loss: 0.68 Running Accuracy: 57.73\n",
      "Batch: 1901 / 4000 Running Loss: 0.68 Running Accuracy: 57.76\n",
      "Batch: 2001 / 4000 Running Loss: 0.68 Running Accuracy: 57.71\n",
      "Batch: 2101 / 4000 Running Loss: 0.68 Running Accuracy: 57.7\n",
      "Batch: 2201 / 4000 Running Loss: 0.68 Running Accuracy: 57.69\n",
      "Batch: 2301 / 4000 Running Loss: 0.68 Running Accuracy: 57.71\n",
      "Batch: 2401 / 4000 Running Loss: 0.68 Running Accuracy: 57.69\n",
      "Batch: 2501 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 2601 / 4000 Running Loss: 0.68 Running Accuracy: 57.66\n",
      "Batch: 2701 / 4000 Running Loss: 0.68 Running Accuracy: 57.68\n",
      "Batch: 2801 / 4000 Running Loss: 0.68 Running Accuracy: 57.69\n",
      "Batch: 2901 / 4000 Running Loss: 0.68 Running Accuracy: 57.7\n",
      "Batch: 3001 / 4000 Running Loss: 0.68 Running Accuracy: 57.69\n",
      "Batch: 3101 / 4000 Running Loss: 0.68 Running Accuracy: 57.69\n",
      "Batch: 3201 / 4000 Running Loss: 0.68 Running Accuracy: 57.68\n",
      "Batch: 3301 / 4000 Running Loss: 0.68 Running Accuracy: 57.7\n",
      "Batch: 3401 / 4000 Running Loss: 0.68 Running Accuracy: 57.72\n",
      "Batch: 3501 / 4000 Running Loss: 0.68 Running Accuracy: 57.71\n",
      "Batch: 3601 / 4000 Running Loss: 0.68 Running Accuracy: 57.7\n",
      "Batch: 3701 / 4000 Running Loss: 0.68 Running Accuracy: 57.71\n",
      "Batch: 3801 / 4000 Running Loss: 0.68 Running Accuracy: 57.69\n",
      "Batch: 3901 / 4000 Running Loss: 0.68 Running Accuracy: 57.69\n",
      "Batch: 1 / 4000 Running Loss: 0.66 Running Accuracy: 62.0\n",
      "Batch: 101 / 4000 Running Loss: 0.68 Running Accuracy: 57.41\n",
      "Batch: 201 / 4000 Running Loss: 0.68 Running Accuracy: 57.36\n",
      "Batch: 301 / 4000 Running Loss: 0.68 Running Accuracy: 57.31\n",
      "Batch: 401 / 4000 Running Loss: 0.68 Running Accuracy: 57.45\n",
      "Batch: 501 / 4000 Running Loss: 0.68 Running Accuracy: 57.39\n",
      "Batch: 601 / 4000 Running Loss: 0.68 Running Accuracy: 57.53\n",
      "Batch: 701 / 4000 Running Loss: 0.68 Running Accuracy: 57.49\n",
      "Batch: 801 / 4000 Running Loss: 0.68 Running Accuracy: 57.49\n",
      "Batch: 901 / 4000 Running Loss: 0.68 Running Accuracy: 57.53\n",
      "Batch: 1001 / 4000 Running Loss: 0.68 Running Accuracy: 57.54\n",
      "Batch: 1101 / 4000 Running Loss: 0.68 Running Accuracy: 57.51\n",
      "Batch: 1201 / 4000 Running Loss: 0.68 Running Accuracy: 57.48\n",
      "Batch: 1301 / 4000 Running Loss: 0.68 Running Accuracy: 57.53\n",
      "Batch: 1401 / 4000 Running Loss: 0.68 Running Accuracy: 57.57\n",
      "Batch: 1501 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 1601 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 1701 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 1801 / 4000 Running Loss: 0.68 Running Accuracy: 57.64\n",
      "Batch: 1901 / 4000 Running Loss: 0.68 Running Accuracy: 57.65\n",
      "Batch: 2001 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 2101 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 2201 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 2301 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 2401 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 2501 / 4000 Running Loss: 0.68 Running Accuracy: 57.56\n",
      "Batch: 2601 / 4000 Running Loss: 0.68 Running Accuracy: 57.55\n",
      "Batch: 2701 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 2801 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 2901 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3001 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 3101 / 4000 Running Loss: 0.68 Running Accuracy: 57.58\n",
      "Batch: 3201 / 4000 Running Loss: 0.68 Running Accuracy: 57.59\n",
      "Batch: 3301 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 3401 / 4000 Running Loss: 0.68 Running Accuracy: 57.63\n",
      "Batch: 3501 / 4000 Running Loss: 0.68 Running Accuracy: 57.6\n",
      "Batch: 3601 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 3701 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Batch: 3801 / 4000 Running Loss: 0.68 Running Accuracy: 57.61\n",
      "Batch: 3901 / 4000 Running Loss: 0.68 Running Accuracy: 57.62\n",
      "Training: Epoch Loss: 0.68 Epoch Accuracy: 57.68\n",
      "Inference: Epoch Loss: 0.68 Epoch Accuracy: 57.61\n",
      "--------------------------------------------------\n",
      "Epoch-No: 8\n",
      "Batch: 1 / 4000 Running Loss: 0.66 Running Accuracy: 63.0\n",
      "Batch: 101 / 4000 Running Loss: 0.68 Running Accuracy: 57.49\n",
      "Batch: 201 / 4000 Running Loss: 0.68 Running Accuracy: 57.32\n",
      "Batch: 301 / 4000 Running Loss: 0.68 Running Accuracy: 57.18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 108\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch-No: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 108\u001b[0m     TE_loss, TE_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_OE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     VE_loss, VE_acc \u001b[38;5;241m=\u001b[39m eval_OE(x_train_tokens,Y_train,loss_fn,optimizer)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, TE_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, TE_acc)\n",
      "Cell \u001b[1;32mIn[50], line 64\u001b[0m, in \u001b[0;36mtrain_OE\u001b[1;34m(x_train_tokens, Y_train, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     62\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 64\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     67\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(track_loss\u001b[38;5;241m/\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m),\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\decorators.py:46\u001b[0m, in \u001b[0;36mdisable\u001b[1;34m(fn, recursive)\u001b[0m\n\u001b[0;32m     44\u001b[0m         fn \u001b[38;5;241m=\u001b[39m innermost_fn(fn)\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m callable(fn)\n\u001b[1;32m---> 46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDisableContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisableContext()\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:437\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m callable(fn)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 437\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py:949\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(filename\u001b[38;5;241m.\u001b[39mendswith(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    947\u001b[0m              importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mEXTENSION_SUFFIXES):\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[0;32m    951\u001b[0m \u001b[38;5;66;03m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n",
      "File \u001b[1;32m<frozen genericpath>:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.h12 = nn.Linear(in_features=180, out_features=150)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=150)\n",
    "        self.h23 = nn.Linear(in_features=150, out_features=110)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=110)\n",
    "        self.h34 = nn.Linear(in_features=110, out_features=80)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=80)\n",
    "        self.h45 = nn.Linear(in_features=80, out_features=50)\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=50)\n",
    "        self.h56 = nn.Linear(in_features=50, out_features=30)\n",
    "        self.bn5 = nn.BatchNorm1d(num_features=30)\n",
    "        self.h67 = nn.Linear(in_features=30,out_features=15)\n",
    "        self.bn6 = nn.BatchNorm1d(num_features=15)\n",
    "        self.h78 = nn.Linear(in_features=15,out_features=6)\n",
    "        self.bn7 = nn.BatchNorm1d(num_features=6)\n",
    "        self.h89 = nn.Linear(in_features=6,out_features=2)\n",
    "        self.bn8 = nn.BatchNorm1d(num_features=2)\n",
    "    def forward(self,x):\n",
    "        x = self.h12(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.h23(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.h34(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.h45(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.act(x)\n",
    "        x = self.h56(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.act(x)\n",
    "        x = self.h67(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.act(x)\n",
    "        x = self.h78(x)\n",
    "        x = self.bn7(x)\n",
    "        x = self.act(x)\n",
    "        x = self.h89(x)\n",
    "        x = self.bn8(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "    \n",
    "def train_OE(x_train_tokens,Y_train,loss_fn,optimizer):\n",
    "    model.train()\n",
    "    track_loss = 0\n",
    "    correct = 0\n",
    "    batches = int(len(x_train_tokens)/batch_size)\n",
    "    for i in range(batches):\n",
    "        training_data = x_train_tokens[batch_size*i:batch_size*(i+1)]\n",
    "        labels = Y_train[batch_size*i:batch_size*(i+1)]\n",
    "        pred = model(training_data)\n",
    "        loss = loss_fn(pred,labels)\n",
    "        track_loss += loss.item()\n",
    "        correct += (torch.argmax(pred,dim=1)==labels).type(torch.float).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i%100==0:\n",
    "            running_loss = round(track_loss/(i+1),2)\n",
    "            running_acc = round((correct/((i+1)*batch_size))*100,2)\n",
    "            print(\"Batch:\", i+1, \"/\",batches, \"Running Loss:\",running_loss, \"Running Accuracy:\",running_acc)\n",
    "        \n",
    "    epoch_loss = track_loss/batches\n",
    "    epoch_acc = (correct/len(x_train_tokens))*100\n",
    "    return round(epoch_loss,2), round(epoch_acc,2)\n",
    "\n",
    "\n",
    "def eval_OE(x_train_tokens,Y_train,loss_fn,optimizer):\n",
    "    model.eval()\n",
    "    track_loss = 0\n",
    "    correct = 0\n",
    "    batches = int(len(x_train_tokens)/batch_size)\n",
    "    for i in range(batches):\n",
    "        training_data = x_train_tokens[batch_size*i:batch_size*(i+1)]\n",
    "        labels = Y_train[batch_size*i:batch_size*(i+1)]\n",
    "        pred = model(training_data)\n",
    "        loss = loss_fn(pred,labels)\n",
    "        track_loss += loss.item()\n",
    "        correct += (torch.argmax(pred,dim=1)==labels).type(torch.float).sum().item()\n",
    "        \n",
    "\n",
    "        if i%100==0:\n",
    "            running_loss = round(track_loss/(i+1),2)\n",
    "            running_acc = round((correct/((i+1)*batch_size))*100,2)\n",
    "            print(\"Batch:\", i+1, \"/\",batches, \"Running Loss:\",running_loss, \"Running Accuracy:\",running_acc)\n",
    "        \n",
    "    epoch_loss = track_loss/batches\n",
    "    epoch_acc = (correct/len(x_train_tokens))*100\n",
    "    return round(epoch_loss,2), round(epoch_acc,2)\n",
    "    \n",
    "\n",
    "model = ANN()\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(params = model.parameters(),lr = lr)\n",
    "epochs = 30\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch-No: {i+1}\")\n",
    "    TE_loss, TE_acc = train_OE(x_train_tokens,Y_train,loss_fn,optimizer)\n",
    "    VE_loss, VE_acc = eval_OE(x_train_tokens,Y_train,loss_fn,optimizer)\n",
    "    print(\"Training:\", \"Epoch Loss:\", TE_loss, \"Epoch Accuracy:\", TE_acc)\n",
    "    print(\"Inference:\", \"Epoch Loss:\", VE_loss, \"Epoch Accuracy:\", VE_acc)\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "Sigmoid: 57.2 %\n",
    "\n",
    "ReLU: 55.7 %\n",
    "\n",
    "BCELoss: 55.3 %\n",
    "\n",
    "Achieved accuracy is not satisfactory. It is maybe because I used a simple ANN for this task. Hence, I am currently working towards doing sentiment analysis using LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
